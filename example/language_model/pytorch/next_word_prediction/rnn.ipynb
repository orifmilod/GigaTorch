{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 9633\n",
      "Number of words: 8600\n",
      "Number of chars 27\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "with open(\"./data.txt\", \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "\n",
    "    data = [line.lower() for line in data]\n",
    "    punctuations = re.compile(r\"[^A-Za-z\\s]+\")\n",
    "    data = [re.sub(punctuations, '', line) for line in data]\n",
    "    data = [line for line in data if line != \"\"]\n",
    "    print(\"Number of sentences:\", len(data))\n",
    "\n",
    "\n",
    "words = {word for sentence in data for word in sentence.split()}\n",
    "# words = {'it', 'weddingring', 'room', 'spot'}\n",
    "num_of_words = len(words)\n",
    "print(\"Number of words:\", num_of_words)\n",
    "word2index = {word: i for i, word in enumerate(words)}\n",
    "index2word = {i: word for i, word in enumerate(words)}\n",
    "\n",
    "chars = set(char for line in data for char in line)\n",
    "num_of_chars = len(chars)\n",
    "print(\"Number of chars\", num_of_chars)\n",
    "batch_size = 1\n",
    "\n",
    "def word2tensor(word):\n",
    "    # That extra 1 dimension is because PyTorch assumes everything is in batches - weâ€™re just using a batch size of 1 here.\n",
    "    # TODO: Check if we can remove the extra dimension or embed more words in the same tensor\n",
    "    tensor = torch.zeros(batch_size, 1, num_of_words)\n",
    "    tensor[0][0][word2index[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "def tensor2word(tensor):\n",
    "    return index2word[tensor.argmax().item()]\n",
    "\n",
    "# def categoryFromOutput(output):\n",
    "#     _, top_i = output.topk(1)\n",
    "#     category_i = top_i[0][0].item()\n",
    "#     return num_of_words[category_i], category_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input2hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden2output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        print(\"Start\", np.nonzero(input.detach().numpy())[-1], hidden)\n",
    "        input = self.input2hidden(input)\n",
    "        print(\"After first layer\", np.nonzero(input.detach().numpy())[-1], hidden)\n",
    "        print(\"1) After input to hidden\", input.shape, hidden.shape)\n",
    "\n",
    "        hidden = self.hidden2hidden(hidden)\n",
    "        print(\"2) After hidden to hidden\", hidden.shape)\n",
    "\n",
    "        hidden = F.tanh(input + hidden)\n",
    "        print(\"3) After tanh\", input.shape, hidden.shape)\n",
    "\n",
    "        output = self.hidden2output(hidden)\n",
    "        print(\"4) After hidden to output\", output.shape)\n",
    "\n",
    "        output = self.softmax(output)\n",
    "        print(\"5) After softmax\", output.shape)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "# Creating data for training \n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "CONTEXT_WINDOW = 3\n",
    "data_context = []\n",
    "\n",
    "for line in data:\n",
    "    line = line.split()\n",
    "    n = len(line)\n",
    "    for i in range(n - CONTEXT_WINDOW):\n",
    "        data_context.append((line[i: i + CONTEXT_WINDOW], line[i + CONTEXT_WINDOW]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start [4390] tensor([[0., 0.]])\n",
      "After first layer [0 1] tensor([[0., 0.]])\n",
      "1) After input to hidden torch.Size([1, 1, 2]) torch.Size([1, 2])\n",
      "2) After hidden to hidden torch.Size([1, 2])\n",
      "3) After tanh torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "4) After hidden to output torch.Size([1, 1, 8600])\n",
      "5) After softmax torch.Size([1, 1, 8600])\n",
      "Start [4993] tensor([[[0.4916, 0.1534]]], grad_fn=<TanhBackward0>)\n",
      "After first layer [0 1] tensor([[[0.4916, 0.1534]]], grad_fn=<TanhBackward0>)\n",
      "1) After input to hidden torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "2) After hidden to hidden torch.Size([1, 1, 2])\n",
      "3) After tanh torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "4) After hidden to output torch.Size([1, 1, 8600])\n",
      "5) After softmax torch.Size([1, 1, 8600])\n",
      "Start [6012] tensor([[[0.2191, 0.1146]]], grad_fn=<TanhBackward0>)\n",
      "After first layer [0 1] tensor([[[0.2191, 0.1146]]], grad_fn=<TanhBackward0>)\n",
      "1) After input to hidden torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "2) After hidden to hidden torch.Size([1, 1, 2])\n",
      "3) After tanh torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "4) After hidden to output torch.Size([1, 1, 8600])\n",
      "5) After softmax torch.Size([1, 1, 8600])\n",
      "Epoch 1/1 Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn = RNN(num_of_words, HIDDEN_LAYER_SIZE, num_of_words)\n",
    "lr = 0.01\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "epochs = 1\n",
    "\n",
    "data_context = data_context[:1]\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in data_context: \n",
    "        hidden = rnn.init_hidden()\n",
    "        rnn.zero_grad()\n",
    "        # print(context, target)\n",
    "        for word in context:\n",
    "            output, hidden = rnn(word2tensor(word), hidden)\n",
    "        \n",
    "        # print(\"Predicted:\", tensor2word(output), \", actual:\", target, \", context:\", context)\n",
    "        # Get rid of the batch size dimension\n",
    "        output = output.squeeze(0)\n",
    "        target_tensor = torch.tensor([word2index[target]], dtype=torch.long)\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Dump output and target tensor to a file output.txt\n",
    "        with open(\"output.txt\", \"a\") as f:\n",
    "            f.write(\"Predicted tensor: \")\n",
    "            output_array = output.detach().numpy()\n",
    "            # Find indices where values are not 0\n",
    "            nonzero_indices = np.nonzero(output_array)[-1]\n",
    "            f.write(str(nonzero_indices) + \"\\n\")\n",
    "\n",
    "            f.write(\"Target tensor: \" + str(target_tensor.detach().numpy()) + \" \" + str(target) + \"\\n\")\n",
    "\n",
    "            f.write(\"Loss: \" + str(loss.item()) + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor(-8.0472, grad_fn=<SumBackward0>)\n",
      "tensor([2])\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor(1.6094, grad_fn=<NllLossBackward0>)\n",
      "tensor([[[-1.1418,  0.4812,  0.1088,  0.0671,  0.2627]]])\n",
      "[0 1 2 3 4]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.zeros(1, 5, requires_grad=True)\n",
    "print(input)\n",
    "input = m(input)\n",
    "print(input)\n",
    "print(input.sum())\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([2])\n",
    "print(target)\n",
    "print(input)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)\n",
    "\n",
    "sample = torch.randn(1, 1, 5)\n",
    "print(sample)\n",
    "# Print non-zero indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
